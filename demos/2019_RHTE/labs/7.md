# 7.0 Debugging Failed Migrations

Most of the time migrations go as planned, but knowing what to do when they don't is critical.  This lab attempts to provide you with a process/guide to follow when investigating a failed migration.

![Failed Migration](./screenshots/lab7/mig-plan-failed.png)

## 7.1 CRD Architecture

![Failed Migration](./screenshots/lab7/mig-custom-resources.png)

It's important to first understand the custom resources in play.  The above architecture diagram illustrates the migration resources and their relationships.  Most of our debugging failed migration executions will focus on the right-side of the digram:
 * MigMigration
 * Velero - Backup and Restore

## 7.1 Workflow

Upon execution of a Migration Plan (stage or migrate), a `MigMigration` is created.  This custom resource is created for each distinct run; and is created on the same cluster where the migration-controller that is orchestrating the migration is running.

Let's take a look at the existing MigMigrations.  On our 4.1 cluster, perform the following:

```bash
oc describe migmigration -n mig
Name:         f2eac100-c82f-11e9-b7e6-93a3075e7890
Namespace:    mig
Labels:       <none>
Annotations:  touch: ed8f2dd3-5da8-4a46-b81a-3a8a5b734aac
API Version:  migration.openshift.io/v1alpha1
Kind:         MigMigration
Metadata:
  Creation Timestamp:  2019-08-26T18:33:07Z
  Generation:          20
  Resource Version:    61141
  Self Link:           /apis/migration.openshift.io/v1alpha1/namespaces/mig/migmigrations/f2eac100-c82f-11e9-b7e6-93a3075e7890
  UID:                 f3047e50-c82f-11e9-891d-0243c5d36d90
Spec:
  Mig Plan Ref:
    Name:        socks-shop-mig-plan
    Namespace:   mig
  Quiesce Pods:  true
  Stage:         false
Status:
  Conditions:
    Category:              Advisory
    Durable:               true
    Last Transition Time:  2019-08-26T18:34:51Z
    Message:               The migration has completed successfully.
    Reason:                Completed
    Status:                True
    Type:                  Succeeded
  Phase:                   Completed
  Start Timestamp:         2019-08-26T18:33:07Z
Events:                    <none>
```
This `MigMigration` describes a successful execution of the socks-shop-mig-plan.

The mig-controller will orchestrate actions on both the source and target clusters.  These actions are as follows:

### 7.1.1 Source Cluster

Two Velero Backup CRs are created:

`Backup #1`:

1. Do an initial backup of k8s resources via Velero (no PV data).
1. Annotate all pods with PVs to track what we want to backup.

`Backup #2`:

1. If quiesce is selected, scale app down to zero:
  *  Scales down to zero, Deployment, DeploymentConfig, Job, Statefulset, etc…..all but pods.   
  * Standalone pods are left alone, hope is there are minimal of these and most people will use Deployment/ReplicaSets so we can scale to zero.  
  * If they had a standalone pod the user is responsible for manual quiesce as they need.


2. Launch ‘stage’ pods, these are used for both stage and migrate, they are a dummy/sleeper pod that just sleeps and mounts the data so we can backup.

3. Do a backup of ‘PV’ data via Velero.


***Note: Velero will sync these Backup CRs between source and target clusters, so they will appear on both clusters.***

### 7.1.2 Target Cluster

Two Velero Restore CRs are created:

`Restore #1`:

1. (Uses Backup #2) -- Restore just the PV data to destination cluster.
  * Do a restore of ‘PV data’, this would be a restore of ‘Backup #2’ above

`Restore #2`:

1. (Uses Backup #1) -- Restore the k8s resources to the destination cluster.

## 7.2 Examining Velero Custom Resources

Let's take a look at these Velero CRs on our 4.1 Cluster:


## 7.3 Controller Logs

Another area we can examine to assist in debugging migration issues is the controller logs.  

### 7.3.1 Migration Controller Logs

### 7.3.2 Velero Controller Logs

### 7.3.3 Restic Controller Logs







Next Lab: [Lab 8 - Migration at Scale via API (optional)](./8.md)<br>
[Home](./README.md)
